<p align="center">
 <img src="./assets/lumina-mgpt-2.0.png" width="15%"/>
 <br>
</p>

<div align="center">
<h1> Lumina-DiMOO: A Unified Discrete Diffusion Model for Multi-Modal Generation and Understanding </h1>

<b>Â¹Shanghai AI Laboratory, &nbsp;  Â²Shanghai Jiao Tong University, &nbsp; Â³Shanghai Innovation Institute</b>

</div>

## ğŸ“š Introduction 
We introduce Lumina-DiMOO, an unified discrete diffusion model, which is designed to perform both multimodal generation and understanding tasks. Additionally, it is capable of handling a variety of other applications, including image editing, subject-driven generation, and controllable synthesis, etc.

## ğŸ”¥ News
**[2025-07-25]** ğŸ‰ğŸ‰ğŸ‰ Lumina-DiMOOâ€™s repository has been built! ğŸ‰ğŸ‰ğŸ‰


## ğŸ“ Open-source Plan
 - [ ] Model Checkpoints
 - [ ] Multi-Modal Generation Inference Code (text-to-image, editing, subject driven, etc.)
 - [ ] Multi-Modal Understanding Inference Code
 - [ ] Training Code 
 - [ ] Technical Report

## ğŸ“½ï¸ Demo Examples
<p align="left">
 <img src="./assets/qualitative.jpg" width="100%"/>
 <br>
</p>


## ğŸ’¬ Discussion
You can reach us with this WeChat QR code!
<p align="left">
 <img src="./assets/wechat.jpg" width="35%"/>
 <br>
</p>

## ğŸ“– BibTeX
```
@misc{lumina-dimoo,
      title={Lumina-DiMOO: A Unified Masked Diffusion Model for Multi-Modal Generation and Understanding},
      author={Alpha VLLM Team},
      year={2025},
      url={https://github.com/Alpha-VLLM/Lumina-DiMOO},
}
```



