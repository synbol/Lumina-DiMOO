<p align="center">
 <img src="./assets/lumina-mgpt-2.0.png" width="15%"/>
 <br>
</p>

<div align="center">
<h1> Lumina-DiMOO: A Unified Discrete Diffusion Model for Multi-Modal Generation and Understanding </h1>

<b>¹Shanghai AI Laboratory, &nbsp;  ²Shanghai Jiao Tong University, &nbsp; ³Shanghai Innovation Institute</b>

</div>

## 📚 Introduction 
We introduce Lumina-DiMOO, an unified discrete diffusion model, which is designed to perform both multimodal generation and understanding tasks. Additionally, it is capable of handling a variety of other applications, including image editing, subject-driven generation, and controllable synthesis, etc.

## 🔥 News
**[2025-07-25]** 🎉🎉🎉 Lumina-DiMOO’s repository has been built! 🎉🎉🎉


## 📝 Open-source Plan
 - [ ] Model Checkpoints
 - [ ] Multi-Modal Generation Inference Code (text-to-image, editing, subject driven, etc.)
 - [ ] Multi-Modal Understanding Inference Code
 - [ ] Training Code 
 - [ ] Technical Report

## 📽️ Demo Examples
<p align="left">
 <img src="./assets/qualitative.jpg" width="100%"/>
 <br>
</p>


## 💬 Discussion
You can reach us with this WeChat QR code!
<p align="left">
 <img src="./assets/wechat.jpg" width="35%"/>
 <br>
</p>

## 📖 BibTeX
```
@misc{lumina-dimoo,
      title={Lumina-DiMOO: A Unified Masked Diffusion Model for Multi-Modal Generation and Understanding},
      author={Alpha VLLM Team},
      year={2025},
      url={https://github.com/Alpha-VLLM/Lumina-DiMOO},
}
```



