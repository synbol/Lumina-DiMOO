<p align="center">
 <img src="./assets/lumina-mgpt-2.0.png" width="15%"/>
 <br>
</p>

<div align="center">
<h1> Lumina-DiMOO: A Unified Masked Diffusion Model for Multi-Modal Generation and Understanding </h1>

<b>Â¹Shanghai AI Laboratory, &nbsp;  Â²Shanghai Jiao Tong University, &nbsp; Â³Shanghai Innovation Institute</b>

</div>

## ğŸ“š Introduction 
We introduce a stand-alone, decoder-only autoregressive model, **trained from scratch**, that unifies a broad spectrum of image generation tasks, including **text-to-image generation, image pair generation, subject-driven generation, multi-turn image editing, controllable generation, and dense prediction**.

## ğŸ”¥ News
**[2025-07-25]** ğŸ‰ğŸ‰ğŸ‰ Lumina-DiMOO 2.0 is released! ğŸ‰ğŸ‰ğŸ‰


## ğŸ“ Open-source Plan
 - [x] Text-to-Image / Image Pair Generation Inference & Checkpoints
 - [x] Finetuning code
 - [ ] All-in-One Inference & Checkpoints
 - [ ] Technical Report

## ğŸ“½ï¸ Demo Examples
<details open>
  <summary>Qualitative Performance</summary>
 <img src="./assets/qualitative.jpg" width="100%"/>
</details>



## ğŸ“– BibTeX
```
@misc{lumina-dimoo,
      title={Lumina-DiMOO: A Unified Masked Diffusion Model for Multi-Modal Generation and Understanding},
      author={Alpha VLLM Team},
      year={2025},
      url={https://github.com/Alpha-VLLM/Lumina-DiMOO},
}
```



