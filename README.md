<p align="center">
 <img src="./assets/lumina-mgpt-2.0.png" width="15%"/>
 <br>
</p>

<div align="center">
<h1> Lumina-DiMOO: A Unified Masked Diffusion Model for Multi-Modal Generation and Understanding </h1>

<b>¹Shanghai AI Laboratory, &nbsp;  ²Shanghai Jiao Tong University, &nbsp; ³Shanghai Innovation Institute</b>

</div>

## 📚 Introduction 
We introduce a stand-alone, decoder-only autoregressive model, **trained from scratch**, that unifies a broad spectrum of image generation tasks, including **text-to-image generation, image pair generation, subject-driven generation, multi-turn image editing, controllable generation, and dense prediction**.

## 🔥 News
**[2025-07-25]** 🎉🎉🎉 Lumina-DiMOO 2.0 is released! 🎉🎉🎉


## 📝 Open-source Plan
 - [x] Text-to-Image / Image Pair Generation Inference & Checkpoints
 - [x] Finetuning code
 - [ ] All-in-One Inference & Checkpoints
 - [ ] Technical Report

## 📽️ Demo Examples
<details open>
  <summary>Qualitative Performance</summary>
 <img src="./assets/qualitative.jpg" width="100%"/>
</details>



## 📖 BibTeX
```
@misc{lumina-dimoo,
      title={Lumina-DiMOO: A Unified Masked Diffusion Model for Multi-Modal Generation and Understanding},
      author={Alpha VLLM Team},
      year={2025},
      url={https://github.com/Alpha-VLLM/Lumina-DiMOO},
}
```



